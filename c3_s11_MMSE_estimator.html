
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Minimum Mean Squared Error (MMSE) Estimation &#8212; Telecom Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'c3_s11_MMSE_estimator';</script>
    <link rel="icon" href="_static/web_favicon.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Maximum a Posteriori (MAP) Estimation" href="c3_s12_MAP_estimator.html" />
    <link rel="prev" title="Bayes Estimation" href="c3_s9_s10_Bayes_estimator.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/web_logo.png" class="logo__image only-light" alt="Telecom Book - Home"/>
    <img src="_static/web_logo.png" class="logo__image only-dark pst-js-only" alt="Telecom Book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">PROBABILITY THEORY</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="pr_c1_s0_probability_intro.html">Introduction to Probability Theory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pr_c1_s1_set_theory.html">Set Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="pr_c1_s2_probability_definition.html">Probability Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="pr_c1_s3_Bayes_theorem.html">Bayes’ Theorem</a></li>

</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pr_c2_s0_random_variable.html">Random Variable</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pr_c2_s1_real_Gaussian_RV_scalar.html">Real Scalar Gaussian Random Variable</a></li>

</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pr_c3_s0_functions_of_RV.html">Functions of A Random Variable</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pr_c3_s1_generation_RV_from_Uniform.html">Generating Random Variables from a Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="pr_c3_s2_method_compute_PDF_func_one_RV.html">Methods to Compute PDF of Functions of A Single RV</a></li>


<li class="toctree-l2"><a class="reference internal" href="pr_c3_s3_Rayleigh_RV.html">Rayleigh Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="pr_c3_s4_Rician_RV.html">Rician Random Variable</a></li>



</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="pr_c4_s0_tail_probability.html">Tail Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="pr_c5_s0_limit_theorem.html">Central Limit Theorem</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pr_c6_s0_multi_random_variables.html">Multiple Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pr_c6_s1_two_RVs.html">Distributions with Two Random Variables</a></li>

<li class="toctree-l2"><a class="reference internal" href="pr_c6_s2_real_Gaussian_RV_vector.html">Real Gaussian Random Vector</a></li>








</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pr_c7_s0_Complex_RV.html">Complex Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pr_c7_s1_complex_Gaussian_RV.html">Multiple Complex Random Variables</a></li>

<li class="toctree-l2"><a class="reference internal" href="pr_c7_s2_ZMCCSG.html">Zero Mean Complex Circularly Symmetric Gaussian (ZMCCSG) RV</a></li>


<li class="toctree-l2"><a class="reference internal" href="pr_c7_s3_complex_Gaussian_vector.html">Standard Complex Gaussian Vector</a></li>





</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pr_c8_s0_Random_Process.html">Random Processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pr_c8_s1_Stochastic_Process.html">Stochastic Process</a></li>






<li class="toctree-l2"><a class="reference internal" href="pr_c8_s2_Gaussian_process_ex_1.html">Example: White Noise Process</a></li>

<li class="toctree-l2"><a class="reference internal" href="pr_c8_s3_Gaussian_process_ex_2.html">Example: Band-Limited White Gaussian Noise Process</a></li>


</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">SIGNAL ANALYSIS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="sig_c1_s0_signal_analysis.html">Signal Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="sig_c1_s1_spectrum.html">Spectrum</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="sig_c2_s0_signal_representations.html">Signal Representations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="sig_c2_s1_complex_lowpass.html">Complex Signal</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c2_s2_signal_characteristics.html">Signal Characteristics</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c2_s3_filter.html">Filter, Channel, and System</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c2_s4_baseband_signal_model.html">Equivalent Baseband Signal Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c2_s5_baseband_channel_model.html">Equivalent Baseband Channel Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c2_s6_baseband_noise_model.html">Equivalent Baseband Noise Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c2_s7_discrete_system.html">Discrete System and Equivalent Discrete Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c2_s8_EbNo_SNR.html">Relationship Between <span class="math notranslate nohighlight">\(E_b/N_0\)</span> and <span class="math notranslate nohighlight">\(\mathtt{SNR}\)</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c2_s9_Sampling_Theory.html">Sampling Theorem for Band-Limited Random Processes</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="sig_c3_s0_vector_space.html">Signal Space</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="sig_c3_s1_signal_space.html">Signal Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c3_s2_Gram_Schmidt_Procedure_for_Signals.html">Gram-Schmidt Procedure for Signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c3_s3_constellation.html">Constellations</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c3_s4_more_on_orthonormality.html">Orthonormal Expansions</a></li>
<li class="toctree-l2"><a class="reference internal" href="sig_c3_s5_KL_expansion.html">Karhunen-Loève (KL) Expansion</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TELECOMMUNICATIONS THEORY</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="com_c1_intro.html">Introduction to Communications Theory</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="com_c2_s0_Modulation.html">Modulation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="com_c2_s1_Memoryless_Modulation.html">Memoryless Modulation Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c2_s2_PAM.html">Pulse Amplitude Modulation (PAM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c2_s3_PSK.html">Phase Modulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c2_s4_QAM.html">Quadrature Amplitude Modulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c2_s5_Multidimensional_Signaling.html">Multidimensional Signaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c2_s6_Orthogonal_Signalling.html">Orthogonal Signaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c2_s7_FSK.html">Frequency-Shift Keying (FSK)</a></li>

<li class="toctree-l2"><a class="reference internal" href="com_c2_s8_Waveform_Binary_Codes.html">Signal Waveforms from Binary Codes</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c2_s9_Signaling_with_Memory.html">Signaling Schemes with Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c2_s10_CPFSK.html">Continuous-Phase Frequency-Shift Keying (CPFSK)</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c2_s11_PSD.html">Power Spectral Density</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="com_c3_s0_Optimal_Receivers.html">Optimal Receivers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="com_c3_s1_General_Vector_Channel_Model.html">Optimal Detection in a General Vector Channel Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c3_s2_MAP_ML_Decision_Regions_Error_Prob.html">MAP and ML Receivers, Decision Regions, and Error Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="com_c3_s3_Efficient_Receiver.html">Efficient Receiver</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DETECTION, DECISION, AND ESTIMATION THEORY</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="c2_s0.html">Fundamentals of Detection Theory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="c2_s5_MAP_ML_critera.html">MAP and ML Criteria</a></li>
<li class="toctree-l2"><a class="reference internal" href="c2_s7_Bayes_criterion.html">Bayes Criterion</a></li>


<li class="toctree-l2"><a class="reference internal" href="c2_s9_minimax.html">MiniMax Criterion</a></li>
<li class="toctree-l2"><a class="reference internal" href="c2_s11_Neyman_Pearson.html">Neyman-Pearson Criterion</a></li>
<li class="toctree-l2"><a class="reference internal" href="c2_x2_LLR_Sionna.html">Tutorial: Differentiable Communication Systems Using Sionna</a></li>

<li class="toctree-l2"><a class="reference internal" href="c2_x3_comparison.html">Perfomance Comaprison</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="c3_s0.html">Fundamentals of Estimation Theory</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="c3_s3_parameter_estimation.html">Parameter Estimation</a></li>


<li class="toctree-l2"><a class="reference internal" href="c3_s5_unbiased_estimator.html">Unbiased Estimates</a></li>
<li class="toctree-l2"><a class="reference internal" href="c3_s6_sufficient_statistic.html">Estimators Based on Sufficient Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="c3_s7_minimum_variance_estimates.html">Minimum Variance Estimaties</a></li>
<li class="toctree-l2"><a class="reference internal" href="c3_s8_consistent_estimator.html">Consistent Estimates</a></li>

<li class="toctree-l2"><a class="reference internal" href="c3_s9_s10_Bayes_estimator.html">Bayes Estimation</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Minimum Mean Squared Error (MMSE) Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="c3_s12_MAP_estimator.html">Maximum a Posteriori (MAP) Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="c3_s13_ML_estimator.html">Maximum Likelihood (ML) Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="c3_s14_comparison_of_estimators.html">Comparison of Estimators</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="c5_s0_intro_copy.html">Vector and Multi-Hypothesis Detection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="c5_s3_Vector_Detection.html">Detection Problem with Multiple Samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="c5_s5_a_Vector_Detection_Criteria.html">Criteria for Multiple Sample Detection of Binary Hypotheses</a></li>
<li class="toctree-l2"><a class="reference internal" href="c5_s5_b_example_a.html">Example: Antipodal Signal Detection using Multiple Samples</a></li>






<li class="toctree-l2"><a class="reference internal" href="c5_s5_c_example_b.html">Example: Detection with Real Additive Gaussian Noise</a></li>




<li class="toctree-l2"><a class="reference internal" href="c5_s6_Optimum_Digital_Detector_Schonhoff.html">The Optimum Digital Detector in Additive Gaussian Noise</a></li>
<li class="toctree-l2"><a class="reference internal" href="c5_s6_Optimum_Digital_Detector_alternative.html">Optimum Digital Detector – Alternative Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="c5_s8_Filters.html">Filtering Alternatives</a></li>
<li class="toctree-l2"><a class="reference internal" href="c5_s12_Optimal_Detection_Continuous_White_Noise.html">Continuous Signals with White Gaussian Noise</a></li>
<li class="toctree-l2"><a class="reference internal" href="c5_s16_Performance_Analysis.html">Perfomance of Binary Receivers in AWGN</a></li>
<li class="toctree-l2"><a class="reference internal" href="c5_s16_Performance_Analysis_ex_5_5.html"><strong>Example 5.5</strong></a></li>


</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="c6_s0_intro.html">Detection of Signals with Random Parameters</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="c6_s1_composite_hypothesis_testing.html">Composite Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="c6_s3_unknown_phase.html">Unknown Phase</a></li>
<li class="toctree-l2"><a class="reference internal" href="c6_s4_unknown_amplitude.html">Unknown Amplitude</a></li>
<li class="toctree-l2"><a class="reference internal" href="c6_s5_unknown_frequency.html">Unknown Frequency</a></li>
<li class="toctree-l2"><a class="reference internal" href="c6_s6_unknown_delay.html">Unknown Delay</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="c7_s0_intro.html">Estimation of Specific Parameters</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="c7_s1_parameter_estimation.html">Parameter Estimation in White Gaussian Noise</a></li>
<li class="toctree-l2"><a class="reference internal" href="c7_s2_amplitude_coherent_estimation.html">Amplitude Estimation in the Coherent Case with AWGN</a></li>
<li class="toctree-l2"><a class="reference internal" href="c7_s3_amplitude_non_coherent_estimation.html">Amplitude Estimation in the Noncoherent Case with AWGN</a></li>
<li class="toctree-l2"><a class="reference internal" href="c7_s4_phase_estimation.html">Phase Estimation in AWGN</a></li>
<li class="toctree-l2"><a class="reference internal" href="c7_s5_delay_estimation.html">Time Delay Estimation in AWGN</a></li>
<li class="toctree-l2"><a class="reference internal" href="c7_s6_frequency_estimation.html">Frequency Estimation in AWGN</a></li>
<li class="toctree-l2"><a class="reference internal" href="c7_s7_simultaneous_parameter_estimation.html">Simultaneous Parameter Estimation in AWGN</a></li>
<li class="toctree-l2"><a class="reference internal" href="c7_s8_multiple_parameters_estimation.html">ML Estimation for a Discrete Linear Observation Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="c7_s9_MAP_Estimation_Discrete_Linear_Observation.html">MAP Estimation for a Discrete Linear Observation Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="c7_s10_Sequential_Parameter_Estimation.html">Sequential Parameter Estimation</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Minimum Mean Squared Error (MMSE) Estimation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-description">Mathematical Description</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alternative-form-of-mse-estimator">Alternative Form of MSE Estimator</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-an-optimal-estimate-for-a-more-general-class-of-cost-functions">Discussion: An optimal estimate for a more general class of cost functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-c3-7-mmse-estimator-bayes-estimator-that-minimize-the-mse-cost-function">Example C3.7: MMSE Estimator (Bayes estimator that minimize the MSE cost function)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-conditional-i-i-d">Discussion. Conditional i.i.d.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-mmmse-estimate">Finding the MMMSE Estimate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation">Simulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-is-this-mmse-estimate-biased">Discussion. Is This MMSE Estimate Biased ?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-mean-and-variance">General Mean and Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-c3-8-average-risk-r-min">Example C3.8: Average Risk <span class="math notranslate nohighlight">\( R_{\min} \)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimum-variance-of-the-estimation-error">Minimum Variance of the Estimation Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">Appendix</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="minimum-mean-squared-error-mmse-estimation">
<h1>Minimum Mean Squared Error (MMSE) Estimation<a class="headerlink" href="#minimum-mean-squared-error-mmse-estimation" title="Link to this heading">#</a></h1>
<p>In Bayesian estimation, when we aim to minimize the mean squared error (MSE) between the true parameter <span class="math notranslate nohighlight">\(\alpha\)</span> and our estimate <span class="math notranslate nohighlight">\(\hat{\alpha}\)</span>, we use the squared-error cost function.</p>
<p>The conditional cost function <span class="math notranslate nohighlight">\(C_{\text{MSE}}(\hat{\alpha}|\vec{y})\)</span> represents the expected squared error given the observations <span class="math notranslate nohighlight">\(\vec{y}\)</span> and is defined as:</p>
<div class="math notranslate nohighlight">
\[
C_{\text{MSE}}(\hat{\alpha}|\vec{y}) = \int_{-\infty}^{\infty} (\alpha - \hat{\alpha}(\vec{y}))^2\, p(\alpha|\vec{y})\, d\alpha
\]</div>
<p>Note that this is not merely a definition; it’s an equation representing how the cost varies with <span class="math notranslate nohighlight">\(\hat{\alpha}\)</span>.</p>
<p>Our goal is to find the estimate <span class="math notranslate nohighlight">\(\hat{\alpha}\)</span> that minimizes this conditional cost function.</p>
<p>To achieve this, from the optimization theory:</p>
<ul class="simple">
<li><p>we treat <span class="math notranslate nohighlight">\(C_{\text{MSE}}(\hat{\alpha}|\vec{y})\)</span> as a function of <span class="math notranslate nohighlight">\(\hat{\alpha}\)</span></p></li>
<li><p>perform optimization by taking its derivative with respect to <span class="math notranslate nohighlight">\(\hat{\alpha}\)</span>,</p></li>
<li><p>setting the derivative equal to zero,</p></li>
<li><p>solving for <span class="math notranslate nohighlight">\(\hat{\alpha}\)</span>.</p></li>
</ul>
<section id="mathematical-description">
<h2>Mathematical Description<a class="headerlink" href="#mathematical-description" title="Link to this heading">#</a></h2>
<p>First, differentiating <span class="math notranslate nohighlight">\(C_{\text{MSE}}(\hat{\alpha}|\vec{y})\)</span> with respect to <span class="math notranslate nohighlight">\(\hat{\alpha}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\hat{\alpha}} C_{\text{MSE}}(\hat{\alpha}|\vec{y}) = \int_{-\infty}^{\infty} \frac{d}{d\hat{\alpha}} \left[(\alpha - \hat{\alpha})^2\right] p(\alpha|\vec{y})\, d\alpha
\]</div>
<p>On the RHS, calculating the derivative inside the integral, we have</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\hat{\alpha}} (\alpha - \hat{\alpha})^2 = -2(\alpha - \hat{\alpha})
\]</div>
<p>Thus, we obtain</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\hat{\alpha}} C_{\text{MSE}}(\hat{\alpha}|\vec{y}) = -2 \int_{-\infty}^{\infty} (\alpha - \hat{\alpha})\, p(\alpha|\vec{y})\, d\alpha
\]</div>
<p>Next, setting the derivative equal to zero to find the minimum:</p>
<div class="math notranslate nohighlight">
\[
0 = -2 \int_{-\infty}^{\infty} (\alpha - \hat{\alpha})\, p(\alpha|\vec{y})\, d\alpha
\]</div>
<p>Simplifying (dividing both sides by <span class="math notranslate nohighlight">\(-2\)</span>):</p>
<div class="math notranslate nohighlight">
\[
0 = \int_{-\infty}^{\infty} (\alpha - \hat{\alpha})\, p(\alpha|\vec{y})\, d\alpha
\]</div>
<p>Next, expanding the integral:</p>
<div class="math notranslate nohighlight">
\[
\int_{-\infty}^{\infty} \alpha\, p(\alpha|\vec{y})\, d\alpha - \hat{\alpha} \int_{-\infty}^{\infty} p(\alpha|\vec{y})\, d\alpha = 0
\]</div>
<p>Recognizing that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} p(\alpha|\vec{y})\, d\alpha = 1\)</span> (since <span class="math notranslate nohighlight">\(p(\alpha|\vec{y})\)</span> is a probability density function)</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\alpha}\)</span> does not depend on <span class="math notranslate nohighlight">\(\alpha\)</span> and can be taken out of the integral</p></li>
</ul>
<p>We get:</p>
<div class="math notranslate nohighlight">
\[
\hat{\alpha} = \int_{-\infty}^{\infty} \alpha\, p(\alpha|\vec{y})\, d\alpha
\]</div>
<p>We observe that the second derivative of <span class="math notranslate nohighlight">\(C_{\text{MSE}}(\hat{\alpha}|\vec{y})\)</span> with respect to <span class="math notranslate nohighlight">\(\hat{\alpha}\)</span> is positive:</p>
<div class="math notranslate nohighlight">
\[
\frac{d^2}{d\hat{\alpha}^2} C_{\text{MSE}}(\hat{\alpha}|\vec{y}) = 2 \int_{-\infty}^{\infty} p(\alpha|\vec{y})\, d\alpha = 2 &gt; 0
\]</div>
<p>This confirms that the critical point found is indeed a minimum.</p>
<p>The right-hand side of the equation is the expected value (mean) of RV <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span> given <span class="math notranslate nohighlight">\(\vec{y}\)</span>, denoted as <span class="math notranslate nohighlight">\(E\{\boldsymbol{\alpha}|\vec{y}\}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[ \boxed{
\hat{\boldsymbol{\alpha}}_{\text{MMSE}} = \int_{-\infty}^{\infty} \alpha\, p(\alpha|\vec{y})\, d\alpha
= E\{\boldsymbol{\alpha}|\vec{y}\}
}
\]</div>
<section id="alternative-form-of-mse-estimator">
<h3>Alternative Form of MSE Estimator<a class="headerlink" href="#alternative-form-of-mse-estimator" title="Link to this heading">#</a></h3>
<p>An alternative form of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\alpha}}_{\text{MMSE}}\)</span> can be obtained by using the identities</p>
<div class="math notranslate nohighlight">
\[
p(\alpha|\vec{y}) = \frac{p(\vec{y}|\alpha)p(\alpha)}{p(\vec{y})}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
p(\vec{y}) = \int_{-\infty}^{\infty} p(\vec{y}|\alpha)p(\alpha) \, d\alpha
\]</div>
<p>resulting in</p>
<div class="math notranslate nohighlight">
\[ \boxed{
\hat{\boldsymbol{\alpha}}_{\text{MMSE}} = \frac{\int_{-\infty}^{\infty} \alpha p(\vec{y}|\alpha)p(\alpha) \, d\alpha}{\int_{-\infty}^{\infty} p(\vec{y}|\alpha)p(\alpha) \, d\alpha}
}
\]</div>
<p>This form requires the pdfs <span class="math notranslate nohighlight">\(p(\vec{y}|\alpha)\)</span> and <span class="math notranslate nohighlight">\(p(\alpha)\)</span> instead of the <em>a posteriori</em> pdf <span class="math notranslate nohighlight">\(p(\alpha|\vec{y})\)</span> to compute <span class="math notranslate nohighlight">\(\hat{\alpha}_{\text{MSE}}\)</span> and is often simpler to evaluate.</p>
</section>
</section>
<section id="discussion-an-optimal-estimate-for-a-more-general-class-of-cost-functions">
<h2>Discussion: An optimal estimate for a more general class of cost functions<a class="headerlink" href="#discussion-an-optimal-estimate-for-a-more-general-class-of-cost-functions" title="Link to this heading">#</a></h2>
<p>In Bayesian estimation, the conditional mean <span class="math notranslate nohighlight">\( E\{\alpha | \vec{y}\} \)</span> is well-known for minimizing the mean squared error (MSE) cost function.</p>
<p>However, its optimality extends to a broader class of cost functions under certain conditions.</p>
<p><strong>First Case: Convex and Symmetric Cost Functions</strong></p>
<ul class="simple">
<li><p><strong>Convexity and Symmetry:</strong> If the cost function <span class="math notranslate nohighlight">\( C(\alpha_e) \)</span> is <em>convex</em> and <em>symmetric</em> with respect to the estimation error <span class="math notranslate nohighlight">\( \boldsymbol{\alpha}_e = \boldsymbol{\alpha} - \hat{\boldsymbol{\alpha}} \)</span>, meaning <span class="math notranslate nohighlight">\(C(\alpha,\hat{\alpha}) = C(\alpha_e)\)</span>, and <span class="math notranslate nohighlight">\( C(\alpha_e) = C(-\alpha_e) \)</span>, then the conditional mean remains the optimal estimator.</p>
<ul>
<li><p><em>Convexity</em> ensures that the cost increases at an increasing rate as the error grows, penalizing larger errors more severely.</p></li>
<li><p><em>Symmetry</em> means that overestimation and underestimation are penalized equally.</p></li>
</ul>
</li>
<li><p><strong>Optimality of Conditional Mean:</strong> Under these conditions, minimizing the expected cost leads to the conditional mean because it balances the errors due to the symmetric nature of both the cost function and the posterior distribution.</p></li>
</ul>
<p><strong>Second Case: Symmetric, Nondecreasing Cost Functions with Specific Posterior Properties</strong></p>
<ul>
<li><p><strong>Symmetric and Nondecreasing Cost Function:</strong> The cost function <span class="math notranslate nohighlight">\( C(\alpha_e) \)</span> is symmetric (<span class="math notranslate nohighlight">\( C(\alpha_e) = C(-\alpha_e) \)</span>) and <strong>nondecreasing</strong> for <span class="math notranslate nohighlight">\( \alpha_e \geq 0 \)</span>, i.e., <span class="math notranslate nohighlight">\(C(\alpha_{e_2}) &gt; C(\alpha_{e_1})\)</span>, for <span class="math notranslate nohighlight">\(\alpha_{e_2} \geq \alpha_{e_1} \geq 0\)</span>. This means that the cost does not decrease as the magnitude of the error increases</p></li>
<li><p><strong>Posterior PDF Conditions:</strong></p>
<ul>
<li><p>The posterior probability density function <span class="math notranslate nohighlight">\( p(\alpha | \vec{y}) \)</span> is <strong>symmetric</strong> about its mean and <strong>unimodal</strong> (has a single peak).</p></li>
<li><p>It satisfies the condition:</p>
<div class="math notranslate nohighlight">
\[
    \lim_{\alpha_e \to \infty} C(\alpha_e) p(\alpha_e | \vec{y}) = 0
    \]</div>
</li>
<li><p>This condition ensures that the product of the cost and the tail of the posterior distribution diminishes to zero, making the expected cost finite and well-defined.</p></li>
</ul>
</li>
<li><p><strong>Optimality of Conditional Mean:</strong> Under these circumstances, the conditional mean minimizes the expected cost because:</p>
<ul class="simple">
<li><p>The symmetry of <span class="math notranslate nohighlight">\( p(\alpha | \vec{y}) \)</span> ensures that the mean is at the center of the distribution.</p></li>
<li><p>The nondecreasing nature of <span class="math notranslate nohighlight">\( C(\alpha_e) \)</span> penalizes larger errors more heavily.</p></li>
<li><p>The diminishing product, i.e., <span class="math notranslate nohighlight">\(\to 0\)</span> as <span class="math notranslate nohighlight">\(m \to \infty\)</span>, guarantees that extreme errors contribute negligibly to the expected cost.</p></li>
</ul>
</li>
</ul>
</section>
<section id="example-c3-7-mmse-estimator-bayes-estimator-that-minimize-the-mse-cost-function">
<h2>Example C3.7: MMSE Estimator (Bayes estimator that minimize the MSE cost function)<a class="headerlink" href="#example-c3-7-mmse-estimator-bayes-estimator-that-minimize-the-mse-cost-function" title="Link to this heading">#</a></h2>
<p><strong>Problem Statement</strong></p>
<p>In this example, based on [B2, Ex 10.7], we want to find the estimate <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\mu}}\)</span> of the unknown random  mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>, using the set of observations <span class="math notranslate nohighlight">\(\mathbf{y}_1, \ldots, \mathbf{y}_m\)</span>.</p>
<p>Assume further that the observations are statistically independent, normal random variables, each with unknown mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and known variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>The conditional pdf <span class="math notranslate nohighlight">\(p(\vec{y}|\mu)\)</span> is then</p>
<div class="math notranslate nohighlight">
\[
p(\vec{y}|\mu) = \frac{1}{(2\pi \sigma^2)^{m/2}} \exp\left(-\frac{1}{2\sigma^2} \sum_{k=1}^{m} (y_k - \mu)^2\right)
\]</div>
<p>Next, assume that the <em>a priori</em> pdf <span class="math notranslate nohighlight">\(p(\mu)\)</span> is normal with mean <span class="math notranslate nohighlight">\(m_1\)</span> and variance <span class="math notranslate nohighlight">\(\beta^2\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[
p(\mu) = \frac{1}{\sqrt{2\pi \beta^2}} \exp\left(-\frac{(\mu - m_1)^2}{2\beta^2}\right)
\]</div>
<p>Note that the prior <span class="math notranslate nohighlight">\( p(\mu) \)</span> represents our knowledge about <span class="math notranslate nohighlight">\(\mu\)</span> before observing any data.</p>
<section id="discussion-conditional-i-i-d">
<h3>Discussion. Conditional i.i.d.<a class="headerlink" href="#discussion-conditional-i-i-d" title="Link to this heading">#</a></h3>
<p>In the example, the mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> is treated as a random variable with a prior distribution <span class="math notranslate nohighlight">\( p(\mu) \)</span>, and yet the observations <span class="math notranslate nohighlight">\( \mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_m \)</span> are considered independent and identically distributed (i.i.d.).</p>
<p>This might seem contradictory at first, but it makes sense within the Bayesian framework due to the concept of <em>conditional independence</em>.</p>
<ul>
<li><p>Conditional Independence Given <span class="math notranslate nohighlight">\(\mu\)</span></p>
<p>The observations <span class="math notranslate nohighlight">\( \mathbf{y}_i \)</span> are <em>conditionally independent</em> given the parameter <span class="math notranslate nohighlight">\(\mu\)</span> (a realization of <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>).</p>
<p>This means that once the value of <span class="math notranslate nohighlight">\(\mu\)</span> is specified, the observations are independent of each other.</p>
<div class="math notranslate nohighlight">
\[
   p(y_1, y_2, \ldots, y_m | \mu) = \prod_{i=1}^m p(y_i | \mu)
   \]</div>
<p>Each <span class="math notranslate nohighlight">\( y_i \)</span> (a relization of RV <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>) is generated from the same distribution <span class="math notranslate nohighlight">\( \mathcal{N}(\mu, \sigma^2) \)</span>, making them identically distributed given <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</li>
<li><p>Unconditional Dependence Due to Random <span class="math notranslate nohighlight">\(\mu\)</span></p>
<p>Dependence through <span class="math notranslate nohighlight">\(\mu\)</span>: Since <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> is a random variable, it introduces dependence among the observations when we consider their joint distribution without conditioning on <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>.</p>
<p>Joint Distribution Without Conditioning:</p>
<div class="math notranslate nohighlight">
\[
   p(y_1, y_2, \ldots, y_m) = \int_{-\infty}^\infty \left( \prod_{i=1}^m p(y_i | \mu) \right) p(\mu) d\mu
   \]</div>
<p>This integral over <span class="math notranslate nohighlight">\(\mu\)</span> mixes the observations together, reflecting their dependence.</p>
</li>
<li><p>Why We Consider i.i.d. in the Likelihood Function:</p>
<p><em>Conditional Likelihood:</em> In Bayesian estimation, we work with the <strong>likelihood function</strong> <span class="math notranslate nohighlight">\( p(\vec{y} | \mu) \)</span>, which treats the observations as independent given <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>This conditional independence allows us to factor the likelihood and simplify the estimation process.</p>
</li>
</ul>
</section>
<section id="finding-the-mmmse-estimate">
<h3>Finding the MMMSE Estimate<a class="headerlink" href="#finding-the-mmmse-estimate" title="Link to this heading">#</a></h3>
<p>In the sequence of this chapter, note that the terms MMSE Estimate and MMMSE Estimate are interchangeable.</p>
<p>Recall that the MSE estimate is defined as</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\alpha}}_{\text{MMSE}} = E\{\boldsymbol{\alpha}|\vec{y}\} = \int_{-\infty}^{\infty} \alpha\, p(\alpha|\vec{y})\, d\alpha
\]</div>
<p>Thus, we need to find the a posteriori <span class="math notranslate nohighlight">\(p(\alpha|\vec{y})\)</span>.</p>
<p>From</p>
<div class="math notranslate nohighlight">
\[
p(\alpha|\vec{y}) = \frac{p(\vec{y}|\alpha)p(\alpha)}{p(\vec{y})}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
p(\vec{y}) = \int_{-\infty}^{\infty} p(\vec{y}|\alpha)p(\alpha) \, d\alpha
\]</div>
<p>the <em>a posteriori</em> pdf can be obtained as</p>
<div class="math notranslate nohighlight">
\[ \boxed{
p(\mu|\vec{y}) = \frac{1}{\sqrt{2\pi \gamma^2}} \exp\left(-\frac{[\mu - \gamma^2 \omega]^2}{2\gamma^2}\right) \qquad \text{(C3.98)}
}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\gamma^2 = \frac{1}{\frac{m}{\sigma^2} + \frac{1}{\beta^2}}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\omega = \frac{m\bar{y}}{\sigma^2} + \frac{m_1}{\beta^2}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\bar{y} = \frac{1}{m} \sum_{k=1}^{m} y_k
\]</div>
<p>See the Appendix for the derivation.</p>
<p>Again, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\alpha}}_{\text{MMSE}} = \int_{-\infty}^{\infty} \alpha\, p(\alpha|\vec{y})\, d\alpha\)</span>, the Bayes estimate can now be obtained as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\mu}_{\text{MMSE}} 
&amp;= \int_{-\infty}^{\infty} \mu \frac{1}{\sqrt{2\pi \gamma^2}} \exp\left(-\frac{1}{2\gamma^2}[\mu - \gamma^2 \omega]^2\right) d\mu
&amp;= \gamma^2 \omega \\
&amp;= \frac{\beta^2 \bar{y} + m_1 \sigma^2 / m}{\beta^2 + \sigma^2 / m}
\end{align*}
\end{split}\]</div>
<p>It follows that the random estimate <span class="math notranslate nohighlight">\(\hat{\mu}_{\text{MMSE}}\)</span> can be written as</p>
<div class="math notranslate nohighlight">
\[ \boxed{
\hat{\boldsymbol{\mu}}_{\text{MMSE}} = \frac{\beta^2 \bar{\mathbf{y}} + m_1 \sigma^2 / m}{\beta^2 + \sigma^2 / m}
}
\]</div>
</section>
<section id="simulation">
<h3>Simulation<a class="headerlink" href="#simulation" title="Link to this heading">#</a></h3>
<p><strong>Objective:</strong></p>
<p>Simulate the Bayesian estimation of an unknown random mean <span class="math notranslate nohighlight">\( \boldsymbol{\mu} \)</span> using observed data <span class="math notranslate nohighlight">\( \mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_m \)</span>.</p>
<p>The true mean <span class="math notranslate nohighlight">\( \boldsymbol{\mu}_{\text{true}} \)</span> is a random variable drawn from the prior distribution <span class="math notranslate nohighlight">\( p(\mu) \)</span>, which is normal with a known, fixed mean <span class="math notranslate nohighlight">\( m_1 \)</span> and a known, fixed variance <span class="math notranslate nohighlight">\( \beta^2 \)</span>.</p>
<p>Each observation <span class="math notranslate nohighlight">\( \mathbf{y}_k \)</span> is then generated from a normal distribution with mean <span class="math notranslate nohighlight">\( \mu_{\text{true}} \)</span> (a realization of <span class="math notranslate nohighlight">\(  \boldsymbol{\mu}_{\text{true}} \)</span>) and known variance <span class="math notranslate nohighlight">\( \sigma^2 \)</span>.</p>
<p>We aim to compute the Bayesian MMSE estimator <span class="math notranslate nohighlight">\( \hat{\mu}_{\text{MMSE}} \)</span> and compare it with the true mean <span class="math notranslate nohighlight">\( \mu_{\text{true}} \)</span>.</p>
<p><strong>Simulation Steps:</strong></p>
<ol class="arabic simple">
<li><p>Set the Parameters:</p>
<ul class="simple">
<li><p>Prior Mean (<span class="math notranslate nohighlight">\( m_1 \)</span>): Mean of the prior distribution.</p></li>
<li><p>Prior Variance (<span class="math notranslate nohighlight">\( \beta^2 \)</span>): Variance of the prior distribution.</p></li>
<li><p>Observation Variance (<span class="math notranslate nohighlight">\( \sigma^2 \)</span>): Known variance of the observations.</p></li>
<li><p>Number of Observations (<span class="math notranslate nohighlight">\( m \)</span>): The number of data points.</p></li>
<li><p>Number of Simulations: To assess estimator performance over multiple trials.</p></li>
</ul>
</li>
<li><p>Generate the True Mean (<span class="math notranslate nohighlight">\( \mu_{\text{true}} \)</span>):</p>
<ul class="simple">
<li><p>Sample <span class="math notranslate nohighlight">\( \mu_{\text{true}} \)</span> from the prior distribution <span class="math notranslate nohighlight">\( \mathcal{N}(m_1, \beta^2) \)</span>.</p></li>
</ul>
</li>
<li><p>Generate Observations:</p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\( k = 1 \)</span> to <span class="math notranslate nohighlight">\( m \)</span>, sample <span class="math notranslate nohighlight">\( y_k \)</span> from <span class="math notranslate nohighlight">\( \mathcal{N}(\mu_{\text{true}}, \sigma^2) \)</span>.</p></li>
</ul>
</li>
<li><p>Compute the Sample Mean (<span class="math notranslate nohighlight">\( \bar{y} \)</span>):</p>
<ul class="simple">
<li><p>Calculate <span class="math notranslate nohighlight">\( \bar{y} = \frac{1}{m} \sum_{k=1}^m y_k \)</span>.</p></li>
</ul>
</li>
<li><p>Compute the Bayesian MMSE Estimator (<span class="math notranslate nohighlight">\( \hat{\mu}_{\text{MMSE}} \)</span>):</p>
<ul class="simple">
<li><p>Use the formula:
$<span class="math notranslate nohighlight">\(
\hat{\mu}_{\text{MMSE}} = \frac{\beta^2 \bar{y} + m_1 \sigma^2 / m}{\beta^2 + \sigma^2 / m}
\)</span>$</p></li>
</ul>
</li>
<li><p>Compare <span class="math notranslate nohighlight">\( \mu_{\text{true}} \)</span> and <span class="math notranslate nohighlight">\( \hat{\mu}_{\text{MMSE}} \)</span>:</p>
<ul class="simple">
<li><p>Calculate the estimation error <span class="math notranslate nohighlight">\( \hat{\mu}_{\text{MMSE}} - \mu_{\text{true}} \)</span>.</p></li>
<li><p>Repeat the simulation multiple times to evaluate the estimator’s performance.</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Parameters</span>
<span class="n">m1</span> <span class="o">=</span> <span class="mf">0.0</span>            <span class="c1"># Prior mean</span>
<span class="n">beta2</span> <span class="o">=</span> <span class="mf">9.0</span>         <span class="c1"># Prior variance (β^2)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span>
<span class="n">sigma2</span> <span class="o">=</span> <span class="mf">4.0</span>        <span class="c1"># Known variance of observations (σ^2)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">10</span>              <span class="c1"># Number of observations</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Number of simulations</span>

<span class="c1"># Arrays to store estimates and true means</span>
<span class="n">mu_true_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mu_estimates_mmse</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mu_estimates_sample_mean</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
    <span class="c1"># Step 2: Generate the true mean mu_true from the prior</span>
    <span class="n">mu_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">mu_true_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mu_true</span><span class="p">)</span>
    
    <span class="c1"># Step 3: Generate observed data y_k ~ N(mu_true, sigma^2)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_true</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
    
    <span class="c1"># Step 4: Compute the sample mean</span>
    <span class="n">y_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Step 5: Compute the Bayesian MMSE estimator</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">y_bar</span> <span class="o">+</span> <span class="n">m1</span> <span class="o">*</span> <span class="n">sigma2</span> <span class="o">/</span> <span class="n">m</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">+</span> <span class="n">sigma2</span> <span class="o">/</span> <span class="n">m</span>
    <span class="n">mu_hat_mmse</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
    
    <span class="c1"># Store the estimates</span>
    <span class="n">mu_estimates_mmse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mu_hat_mmse</span><span class="p">)</span>
    <span class="n">mu_estimates_sample_mean</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_bar</span><span class="p">)</span>

<span class="c1"># Convert lists to arrays</span>
<span class="n">mu_true_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mu_true_values</span><span class="p">)</span>
<span class="n">mu_estimates_mmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mu_estimates_mmse</span><span class="p">)</span>
<span class="n">mu_estimates_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mu_estimates_sample_mean</span><span class="p">)</span>

<span class="c1"># Step 6: Compute the Mean Squared Errors</span>
<span class="n">mse_bayes_mmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">mu_estimates_mmse</span> <span class="o">-</span> <span class="n">mu_true_values</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mse_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">mu_estimates_sample_mean</span> <span class="o">-</span> <span class="n">mu_true_values</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bayesian MMSE Estimator Mean Squared Error: </span><span class="si">{</span><span class="n">mse_bayes_mmse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample Mean Estimator Mean Squared Error: </span><span class="si">{</span><span class="n">mse_sample_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plotting the estimation errors</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Histogram of errors for Bayesian MMSE estimator</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">errors_bayes_mmse</span> <span class="o">=</span> <span class="n">mu_estimates_mmse</span> <span class="o">-</span> <span class="n">mu_true_values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">errors_bayes_mmse</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Errors of Bayesian MMSE Estimator&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Estimation Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>

<span class="c1"># Histogram of errors for Sample Mean estimator</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">errors_sample_mean</span> <span class="o">=</span> <span class="n">mu_estimates_sample_mean</span> <span class="o">-</span> <span class="n">mu_true_values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">errors_sample_mean</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Errors of Sample Mean Estimator&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Estimation Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Bayesian MMSE Estimator Mean Squared Error: 0.3754
Sample Mean Estimator Mean Squared Error: 0.3826
</pre></div>
</div>
<img alt="_images/965e9a034f5ce91c3a1f67fdf5dda3f1833afb54997ede447ec584678dbb2220.png" src="_images/965e9a034f5ce91c3a1f67fdf5dda3f1833afb54997ede447ec584678dbb2220.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Parameters</span>
<span class="n">m1</span> <span class="o">=</span> <span class="mf">0.0</span>            <span class="c1"># Prior mean</span>
<span class="n">beta2</span> <span class="o">=</span> <span class="mf">9.0</span>         <span class="c1"># Prior variance (β^2)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span>
<span class="n">sigma2</span> <span class="o">=</span> <span class="mf">4.0</span>        <span class="c1"># Known variance of observations (σ^2)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="p">)</span>
<span class="n">m_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>  <span class="c1"># Different values of m</span>
<span class="n">num_simulations</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Number of simulations for each m</span>

<span class="c1"># Arrays to store results</span>
<span class="n">mean_abs_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">std_abs_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mse_errors</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">m_values</span><span class="p">:</span>
    <span class="n">estimation_errors</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
        <span class="c1"># Step 2: Generate the true mean mu_true from the prior</span>
        <span class="n">mu_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        
        <span class="c1"># Step 3: Generate observed data y_k ~ N(mu_true, sigma^2)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_true</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
        
        <span class="c1"># Step 4: Compute the sample mean</span>
        <span class="n">y_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># Step 5: Compute the Bayesian MMSE estimator</span>
        <span class="n">numerator</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">y_bar</span> <span class="o">+</span> <span class="n">m1</span> <span class="o">*</span> <span class="n">sigma2</span> <span class="o">/</span> <span class="n">m</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">+</span> <span class="n">sigma2</span> <span class="o">/</span> <span class="n">m</span>
        <span class="n">mu_hat_mmse</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
        
        <span class="c1"># Compute the estimation error</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">mu_hat_mmse</span> <span class="o">-</span> <span class="n">mu_true</span>
        <span class="n">estimation_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    
    <span class="c1"># Convert list to array</span>
    <span class="n">estimation_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">estimation_errors</span><span class="p">)</span>
    
    <span class="c1"># Compute mean absolute error and standard deviation</span>
    <span class="n">mean_abs_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">estimation_errors</span><span class="p">))</span>
    <span class="n">std_abs_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">estimation_errors</span><span class="p">))</span>
    <span class="n">mse_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">estimation_errors</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Store the results</span>
    <span class="n">mean_abs_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_abs_error</span><span class="p">)</span>
    <span class="n">std_abs_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">std_abs_error</span><span class="p">)</span>
    <span class="n">mse_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse_error</span><span class="p">)</span>

<span class="c1"># Plotting Mean Absolute Estimation Error vs. m</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">m_values</span><span class="p">,</span> <span class="n">mean_abs_errors</span><span class="p">,</span> 
             <span class="n">yerr</span><span class="o">=</span><span class="n">std_abs_errors</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Mean Absolute Estimation Error vs. Number of Observations (m)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Observations (m)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Absolute Estimation Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plotting Mean Squared Error vs. m</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">m_values</span><span class="p">,</span> <span class="n">mse_errors</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error vs. Number of Observations (m)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Observations (m)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/aff0fb05afb7f6f940de293d69755d04a3a26ded67e14b29caae7191e3ef331c.png" src="_images/aff0fb05afb7f6f940de293d69755d04a3a26ded67e14b29caae7191e3ef331c.png" />
<img alt="_images/feb9df1c39fc51f33a741b6264256673e97744db6cb27ed16cd2e661c6abbd4b.png" src="_images/feb9df1c39fc51f33a741b6264256673e97744db6cb27ed16cd2e661c6abbd4b.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fixed true mean</span>
<span class="n">mu_true_random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="n">mu_true</span> <span class="o">=</span> <span class="n">mu_true_random</span>  <span class="c1"># Fixed true mean for all simulations</span>

<span class="c1"># Arrays to store results</span>
<span class="n">average_mu_hat_mmse</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">std_mu_hat_mmse</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">m_values</span><span class="p">:</span>
    <span class="n">mu_hat_mmse_values</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
        <span class="c1"># Generate observed data y_k ~ N(mu_true, sigma^2)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_true</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
        
        <span class="c1"># Compute the sample mean</span>
        <span class="n">y_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># Compute the Bayesian MMSE estimator</span>
        <span class="n">numerator</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">y_bar</span> <span class="o">+</span> <span class="n">m1</span> <span class="o">*</span> <span class="n">sigma2</span> <span class="o">/</span> <span class="n">m</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">+</span> <span class="n">sigma2</span> <span class="o">/</span> <span class="n">m</span>
        <span class="n">mu_hat_mmse</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
        
        <span class="c1"># Store the estimator</span>
        <span class="n">mu_hat_mmse_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mu_hat_mmse</span><span class="p">)</span>
    
    <span class="c1"># Convert list to array</span>
    <span class="n">mu_hat_mmse_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mu_hat_mmse_values</span><span class="p">)</span>
    
    <span class="c1"># Compute average and standard deviation of mu_hat_mmse</span>
    <span class="n">avg_mu_hat_mmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mu_hat_mmse_values</span><span class="p">)</span>
    <span class="n">std_mu_hat_mmse_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mu_hat_mmse_values</span><span class="p">)</span>
    
    <span class="c1"># Store the results</span>
    <span class="n">average_mu_hat_mmse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_mu_hat_mmse</span><span class="p">)</span>
    <span class="n">std_mu_hat_mmse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">std_mu_hat_mmse_val</span><span class="p">)</span>

<span class="c1"># Plotting mu_true and average mu_hat_mmse vs. m</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">m_values</span><span class="p">,</span> <span class="n">average_mu_hat_mmse</span><span class="p">,</span> 
             <span class="n">yerr</span><span class="o">=</span><span class="n">std_mu_hat_mmse</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o-&#39;</span><span class="p">,</span> 
             <span class="n">capsize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Average m-hat-MS&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">mu_true</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;mu-true&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Average Bayesian MMSE Estimator</span><span class="se">\n</span><span class="s1">vs. Number of Observations (m)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Observations (m)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Estimator Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f37bbe97065e809b3d998a9cf8cd314da15637776ff86dac860c29fa56c084ae.png" src="_images/f37bbe97065e809b3d998a9cf8cd314da15637776ff86dac860c29fa56c084ae.png" />
</div>
</div>
</section>
<section id="discussion-is-this-mmse-estimate-biased">
<h3>Discussion. Is This MMSE Estimate Biased ?<a class="headerlink" href="#discussion-is-this-mmse-estimate-biased" title="Link to this heading">#</a></h3>
<p>Computing the expected value of this estimate, assuming that the parameter <span class="math notranslate nohighlight">\(\mu\)</span> is held constant (a realization of the unknown random parameter), results in</p>
<div class="math notranslate nohighlight">
\[
E\{\hat{\mu}_{\text{MMSE}}|\mu\} = \frac{\beta^2 E\{\vec{y}|\mu\} + m_1 \sigma^2 / m}{\beta^2 + \sigma^2 / m}
\]</div>
<p>But the expected value of the sample mean <span class="math notranslate nohighlight">\(\vec{y}\)</span> for constant <span class="math notranslate nohighlight">\(\mu\)</span> is <span class="math notranslate nohighlight">\(E\{\vec{y}|\mu\} = \mu\)</span>, so that</p>
<div class="math notranslate nohighlight">
\[
E\{\hat{\mu}_{\text{MMSE}}|\mu\} = \frac{\beta^2 \mu + m_1 \sigma^2 / m}{\beta^2 + \sigma^2 / m}
\]</div>
<p>Since the expected value <span class="math notranslate nohighlight">\(E\{\hat{\mu}_{\text{MMSE}}|\mu\}\)</span> is not equal to <span class="math notranslate nohighlight">\(\mu\)</span>, the estimate is <strong>biased</strong>.</p>
<p>However, for a large number of observations—i.e., as <span class="math notranslate nohighlight">\(m\)</span> becomes large—the estimate is <strong>asymptotically unbiased</strong>.</p>
<p>To see this, we take the expectation of <span class="math notranslate nohighlight">\(E\{\hat{\mu}_{\text{MMSE}}|\mu\}\)</span> again over the random variable <span class="math notranslate nohighlight">\(\mu\)</span>, which can be stated as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
E\{E\{\hat{\mu}_{\text{MMSE}}|\mu\}\} &amp;= \frac{\beta^2 E\{\mu\} + m_1 \sigma^2 / m}{\beta^2 + \sigma^2 / m} \\
&amp;= m_1 \\
&amp;= E\{\mu\}
\end{align*}
\end{split}\]</div>
<p>where the first expectation is with respect to <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</section>
</section>
<section id="general-mean-and-variance">
<h2>General Mean and Variance<a class="headerlink" href="#general-mean-and-variance" title="Link to this heading">#</a></h2>
<p>This last result is computed for a special case but is actually true in general.</p>
<p>To prove this result, the expected value of the estimate is obtained, i.e.,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
E\{\hat{\alpha}_{\text{MSE}}\} 
&amp;= E_{\vec{y}}\{E\{\hat{\alpha}|\vec{y}\}\} \\
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \alpha p(\alpha|\vec{y}) p(\vec{y}) d\vec{y} d\alpha \\
&amp;= \int_{-\infty}^{\infty} \alpha d\alpha \int_{-\infty}^{\infty} p(\alpha, \vec{y}) d\vec{y} \\
&amp;= \int_{-\infty}^{\infty} \alpha p(\alpha) d\alpha \\
&amp;= E\{\alpha\}
\end{align*}
\end{split}\]</div>
<p>From Eq. (C3.78) and the foregoing result, it can be seen that <span class="math notranslate nohighlight">\(E\{\alpha_e\} = 0\)</span> for the MSE cost function.</p>
<p>Therefore,</p>
<ul class="simple">
<li><p>the conditional cost in Eq. (C3.80) is the conditional error variance, and</p></li>
<li><p>the average risk in Eq. (C3.81) is the variance of the estimation error or error variance.</p></li>
</ul>
<p>The relationship can be more easily seen by writing the variance of the estimation error <span class="math notranslate nohighlight">\(V\{\alpha_e\}\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
V\{\alpha_e\} 
&amp;= E\{[\alpha_e - E\{\alpha_e\}]^2\} \\
&amp;= E\{\alpha_e^2\} \\
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} [\alpha - \hat{\alpha}(\vec{y})]^2 p(\alpha, \vec{y}) d\alpha d\vec{y}
\end{align*}
\end{split}\]</div>
<p>This equation is actually the average risk with an MSE cost function.</p>
<p>Since the error variance is minimized using the MSE cost function, the estimate obtained is denoted <span class="math notranslate nohighlight">\(\hat{\alpha}_{\text{MSE}}\)</span> and is referred to as a minimum error-variance estimate.</p>
</section>
<section id="example-c3-8-average-risk-r-min">
<h2>Example C3.8: Average Risk <span class="math notranslate nohighlight">\( R_{\min} \)</span><a class="headerlink" href="#example-c3-8-average-risk-r-min" title="Link to this heading">#</a></h2>
<p>The previous example can be continued by substituting the estimate into Eq. (C3.79) to obtain the average risk, <span class="math notranslate nohighlight">\( R_{\min} \)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
R_{\min} 
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (\mu - \hat{\mu})^2 p(\mu|\vec{y}) p(\vec{y}) d\mu d\vec{y} \\
&amp;= \int_{-\infty}^{\infty} p(\vec{y}) \int_{-\infty}^{\infty} (\mu - \gamma^2 \omega)^2 \frac{1}{\sqrt{2\pi \gamma^2}} \exp\left(-\frac{[\mu - \gamma^2 \omega]^2}{2\gamma^2}\right) d\mu d\vec{y} \\
&amp;= \int_{-\infty}^{\infty} p(\vec{y}) \gamma^2 d\vec{y} = \gamma^2 \\
&amp;= \frac{1}{\frac{m}{\sigma^2} + \frac{1}{\beta^2}} \\
&amp;= \frac{\sigma^2 / m}{\left(\frac{\beta^2}{\beta^2 + \sigma^2 / m}\right)}
\end{align*}
\end{split}\]</div>
</section>
<section id="minimum-variance-of-the-estimation-error">
<h2>Minimum Variance of the Estimation Error<a class="headerlink" href="#minimum-variance-of-the-estimation-error" title="Link to this heading">#</a></h2>
<p>The above expression of <span class="math notranslate nohighlight">\(R_{\min}\)</span> is the minimum variance of the estimation error.</p>
<p>An alternate derivation of the preceding equation can be obtained by computing the variance of the estimation error using</p>
<div class="math notranslate nohighlight">
\[
V\{\mu_e\} = E\{\mu_e^2\} - [E\{\mu_e\}]^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_e = \mu - \hat{\mu}_{\text{MMSE}}\)</span>. Note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
E\{\mu_e\} &amp;= E\{\mu - \hat{\mu}_{\text{MMSE}}\} \\
&amp;= E\{\mu\} - E\{E\{\hat{\mu}_{\text{MMSE}}|\mu\}\} \\
&amp;= m_1 - m_1 = 0
\end{align*}
\end{split}\]</div>
<p>The error variance is then</p>
<div class="math notranslate nohighlight">
\[
V\{\mu_e\} = E\{\mu_e^2\} = E\{E[(\mu - \hat{\mu}_{\text{MMSE}})^2|\mu]\}
\]</div>
<p>Examining the inner expectation results in</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
V\{\mu_e|\mu\} &amp;= E\{(\mu - \hat{\mu}_{\text{MMSE}})^2|\mu\} \\
&amp;= E\{(\mu^2 - 2\mu \hat{\mu}_{\text{MMSE}} + \hat{\mu}_{\text{MMSE}}^2)|\mu\} \\
&amp;= \mu^2 - 2\mu E\{\hat{\mu}_{\text{MMSE}}|\mu\} + E\{\hat{\mu}_{\text{MMSE}}^2|\mu\}
\end{align*}
\end{split}\]</div>
<p>Since</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu}_{\text{MMSE}}^2 = \frac{\beta^4 \gamma^2 + 2m_1 \beta^2 (\sigma^2 / m) \vec{y} + m_1^2 \sigma^4 / m^2}{(\beta^2 + \sigma^2 / m)^2}
\]</div>
<p>it follows that</p>
<div class="math notranslate nohighlight">
\[
E\{\hat{\mu}_{\text{MMSE}}^2|\mu\} = \frac{\beta^4 E\{\vec{y}^2|\mu\} + 2m_1 \beta^2 (\sigma^2 / m) E\{\vec{y}|\mu\} + m_1^2 \sigma^4 / m^2}{(\beta^2 + \sigma^2 / m)^2}
\]</div>
<p>Recognizing <span class="math notranslate nohighlight">\( V\{\vec{y}|\mu\} = \sigma^2 / m \)</span>, where <span class="math notranslate nohighlight">\( E\{\vec{y}|\mu\} = \mu \)</span>, it can be seen that <span class="math notranslate nohighlight">\( E\{\vec{y}^2|\mu\} = \frac{\sigma^2}{m} + \mu^2 \)</span>, so that the previous equation can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
V\{\mu_e|\mu\} &amp;= E\bigg\{\mu^2 - 2\mu\left(\frac{\beta^2 \mu + m_1 \sigma^2 / m}{\beta^2 + \sigma^2 / m}\right) \\
&amp;\qquad+ \frac{\beta^4 \left(\frac{\sigma^2}{m} + \mu^2\right) + 2m_1 \beta^2 (\sigma^2 / m) \mu + m_1^2 \sigma^4 / m^2}{(\beta^2 + \sigma^2 / m)^2}\Bigg|\mu\bigg\}
\end{align*}
\end{split}\]</div>
<p>and, eliminating the condition on <span class="math notranslate nohighlight">\(\mu\)</span>, the equation becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
V\{\mu_e\} &amp;= E\bigg\{\mu^2 - 2\mu\left(\frac{\beta^2 \mu + m_1 \sigma^2 / m}{\beta^2 + \sigma^2 / m}\right) \\
&amp;\qquad + \frac{\beta^4 \left(\frac{\sigma^2}{m} + \mu^2\right) + 2m_1 \beta^2 (\sigma^2 / m) \mu + m_1^2 \sigma^4 / m^2}{(\beta^2 + \sigma^2 / m)^2}\bigg\}
\end{align*}
\end{split}\]</div>
<p>Using <span class="math notranslate nohighlight">\( E\{\mu^2\} = \beta^2 + m_1^2 \)</span> and <span class="math notranslate nohighlight">\( E\{\mu\} = m_1 \)</span>, the preceding equation can be reduced to</p>
<div class="math notranslate nohighlight">
\[
V\{\mu_e\} = \frac{\beta^2 \sigma^2 / m}{\beta^2 + \sigma^2 / m}
\]</div>
<p>This result is the minimum variance of the estimation error expressed as <span class="math notranslate nohighlight">\( R_{\min} \)</span> in Example C3.8 above.</p>
<p>Note that the variance of the estimate <span class="math notranslate nohighlight">\( V\{\hat{\mu}_{\text{MMSE}}\} \)</span> can also be computed, i.e.,</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\begin{align*}
V\{\hat{\mu}_{\text{MMSE}}\} 
&amp;= V\left\{\frac{\beta^2 \vec{y} + m_1 \sigma^2 / m}{\beta^2 + \sigma^2 / m}\right\} \\
&amp;= \left(\frac{\beta^2}{\beta^2 + \sigma^2 / m}\right)^2 V(\vec{y}) \\
&amp;= \frac{\sigma^2 / m \beta^4}{(\beta^2 + \sigma^2 / m)^2}
\end{align*}
\end{split}\]</div>
</section>
<section id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Link to this heading">#</a></h2>
<p><strong>Derivation of Equation (C3.98):</strong></p>
<p>We derive the posterior probability density function (pdf) <span class="math notranslate nohighlight">\( p(\mu | \vec{y}) \)</span> given by:</p>
<div class="math notranslate nohighlight">
\[
p(\mu | \vec{y}) = \frac{1}{\sqrt{2\pi \gamma^2}} \exp\left( -\frac{[\mu - \gamma^2 \omega]^2}{2\gamma^2} \right)
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \gamma^2 = \left( \dfrac{m}{\sigma^2} + \dfrac{1}{\beta^2} \right)^{-1} \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \omega = \dfrac{m \bar{y}}{\sigma^2} + \dfrac{m_1}{\beta^2} \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \bar{y} = \dfrac{1}{m} \sum_{k=1}^{m} y_k \)</span></p></li>
</ul>
<p>Given that, the observations <span class="math notranslate nohighlight">\( \mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_m \)</span> are independent and normally distributed with mean <span class="math notranslate nohighlight">\( \mu \)</span> and known variance <span class="math notranslate nohighlight">\( \sigma^2 \)</span>, and <span class="math notranslate nohighlight">\(\vec{y}\)</span> is one realization of the observation set:</p>
<div class="math notranslate nohighlight">
\[
p(\vec{y} | \mu) = \frac{1}{(2\pi \sigma^2)^{m/2}} \exp\left( -\frac{1}{2\sigma^2} \sum_{k=1}^{m} (y_k - \mu)^2 \right)
\]</div>
<p>The prior pdf of <span class="math notranslate nohighlight">\( \mu \)</span> is normal with mean <span class="math notranslate nohighlight">\( m_1 \)</span> and variance <span class="math notranslate nohighlight">\( \beta^2 \)</span>:</p>
<div class="math notranslate nohighlight">
\[
p(\mu) = \frac{1}{\sqrt{2\pi \beta^2}} \exp\left( -\frac{(\mu - m_1)^2}{2\beta^2} \right)
\]</div>
<p>The posterior pdf is given by:</p>
<div class="math notranslate nohighlight">
\[
p(\mu | \vec{y}) = \frac{p(\vec{y} | \mu) p(\mu)}{p(\vec{y})}
\]</div>
<p>Since <span class="math notranslate nohighlight">\( p(\vec{y}) \)</span> does not depend on <span class="math notranslate nohighlight">\( \mu \)</span>, we can write:</p>
<div class="math notranslate nohighlight">
\[
p(\mu | \vec{y}) \propto p(\vec{y} | \mu) p(\mu)
\]</div>
<p>We focus on the numerator because the denominator <span class="math notranslate nohighlight">\( p(\vec{y}) \)</span> serves as a normalization constant.</p>
<p>Combine the exponents from the likelihood and</p>
<div class="math notranslate nohighlight">
\[
-\frac{1}{2\sigma^2} \sum_{k=1}^{m} (y_k - \mu)^2
\]</div>
<p>and the prior exponent</p>
<div class="math notranslate nohighlight">
\[
-\frac{(\mu - m_1)^2}{2\beta^2}
\]</div>
<p>We have the likelihood term expansion as</p>
<div class="math notranslate nohighlight">
\[
\sum_{k=1}^{m} (y_k - \mu)^2 = \sum_{k=1}^{m} \left( y_k^2 - 2 y_k \mu + \mu^2 \right) = \sum_{k=1}^{m} y_k^2 - 2 \mu \sum_{k=1}^{m} y_k + m \mu^2
\]</div>
<p>and the prior term expansion as</p>
<div class="math notranslate nohighlight">
\[
(\mu - m_1)^2 = \mu^2 - 2 \mu m_1 + m_1^2
\]</div>
<p>The combined exponent <span class="math notranslate nohighlight">\( S \)</span> is:</p>
<div class="math notranslate nohighlight">
\[
S = -\frac{1}{2\sigma^2} \left( \sum_{k=1}^{m} y_k^2 - 2 \mu \sum_{k=1}^{m} y_k + m \mu^2 \right) - \frac{1}{2\beta^2} \left( \mu^2 - 2 \mu m_1 + m_1^2 \right)
\]</div>
<p>Combining the terms involving <span class="math notranslate nohighlight">\( \mu^2 \)</span>, <span class="math notranslate nohighlight">\( \mu \)</span>, and constants, we have</p>
<div class="math notranslate nohighlight">
\[
-\frac{m}{2\sigma^2} - \frac{1}{2\beta^2} = -\frac{1}{2} \left( \frac{m}{\sigma^2} + \frac{1}{\beta^2} \right)
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{1}{\sigma^2} \sum_{k=1}^{m} y_k + \frac{m_1}{\beta^2}
\]</div>
<p>We can denote them collectively as <span class="math notranslate nohighlight">\( C \)</span>, which will be absorbed into the normalization constant.</p>
<p>Thus, we have</p>
<div class="math notranslate nohighlight">
\[
S = -\frac{1}{2} \left( \frac{m}{\sigma^2} + \frac{1}{\beta^2} \right) \mu^2 + \left( \frac{1}{\sigma^2} \sum_{k=1}^{m} y_k + \frac{m_1}{\beta^2} \right) \mu + C
\]</div>
<p>To express <span class="math notranslate nohighlight">\( S \)</span> in the form <span class="math notranslate nohighlight">\( -\dfrac{1}{2\gamma^2} (\mu - \mu_{\text{post}})^2 \)</span>, we complete the square.</p>
<p>Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( a = \left( \dfrac{m}{\sigma^2} + \dfrac{1}{\beta^2} \right) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( b = \left( \dfrac{1}{\sigma^2} \sum_{k=1}^{m} y_k + \dfrac{m_1}{\beta^2} \right) \)</span></p></li>
</ul>
<p>We have</p>
<div class="math notranslate nohighlight">
\[
S = -\frac{a}{2} \mu^2 + b \mu + C
\]</div>
<p>and then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
S &amp;= -\frac{a}{2} \left( \mu^2 - \frac{2b}{a} \mu \right) + C \\
&amp;= -\frac{a}{2} \left[ \left( \mu - \frac{b}{a} \right)^2 - \left( \frac{b}{a} \right)^2 \right] + C \\
&amp;= -\frac{a}{2} \left( \mu - \frac{b}{a} \right)^2 + \frac{b^2}{2a} + C
\end{align*}
\end{split}\]</div>
<p>The term <span class="math notranslate nohighlight">\( \frac{b^2}{2a} + C \)</span> is a constant with respect to <span class="math notranslate nohighlight">\( \mu \)</span> and can be absorbed into the normalization constant.</p>
<p>Next, identify <span class="math notranslate nohighlight">\( \gamma^2 \)</span> and <span class="math notranslate nohighlight">\( \omega \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\gamma^2 = \frac{1}{a} = \left( \frac{m}{\sigma^2} + \frac{1}{\beta^2} \right)^{-1}
\]</div>
<div class="math notranslate nohighlight">
\[
\mu_{\text{post}} = \frac{b}{a} = \gamma^2 \left( \frac{1}{\sigma^2} \sum_{k=1}^{m} y_k + \frac{m_1}{\beta^2} \right) = \gamma^2 \omega
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\omega = \frac{1}{\sigma^2} \sum_{k=1}^{m} y_k + \frac{m_1}{\beta^2} = \frac{m \bar{y}}{\sigma^2} + \frac{m_1}{\beta^2}
\]</div>
<p>since <span class="math notranslate nohighlight">\( \sum_{k=1}^{m} y_k = m \bar{y} \)</span>.</p>
<p>Substituting back into the exponent:</p>
<div class="math notranslate nohighlight">
\[
S = -\frac{1}{2 \gamma^2} \left( \mu - \gamma^2 \omega \right)^2 + \text{constants}
\]</div>
<p>Therefore, the posterior pdf is:</p>
<div class="math notranslate nohighlight">
\[
p(\mu | \vec{y}) = \frac{1}{\sqrt{2\pi \gamma^2}} \exp\left( -\frac{[\mu - \gamma^2 \omega]^2}{2\gamma^2} \right)
\]</div>
<p>We obtain (C3.98).</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="c3_s9_s10_Bayes_estimator.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Bayes Estimation</p>
      </div>
    </a>
    <a class="right-next"
       href="c3_s12_MAP_estimator.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Maximum a Posteriori (MAP) Estimation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-description">Mathematical Description</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alternative-form-of-mse-estimator">Alternative Form of MSE Estimator</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-an-optimal-estimate-for-a-more-general-class-of-cost-functions">Discussion: An optimal estimate for a more general class of cost functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-c3-7-mmse-estimator-bayes-estimator-that-minimize-the-mse-cost-function">Example C3.7: MMSE Estimator (Bayes estimator that minimize the MSE cost function)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-conditional-i-i-d">Discussion. Conditional i.i.d.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-mmmse-estimate">Finding the MMMSE Estimate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation">Simulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-is-this-mmse-estimate-biased">Discussion. Is This MMSE Estimate Biased ?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-mean-and-variance">General Mean and Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-c3-8-average-risk-r-min">Example C3.8: Average Risk <span class="math notranslate nohighlight">\( R_{\min} \)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimum-variance-of-the-estimation-error">Minimum Variance of the Estimation Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">Appendix</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Telecom Book
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>